{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdbe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG System Demo\n",
    "\n",
    "This notebook demonstrates how to use the Retrieval Augmented Generation (RAG) system to query documents.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary functions and dependencies:\n",
    "\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('../')  # Adjust if needed to find the parent directory\n",
    "\n",
    "from query_data import query_rag\n",
    "from populate_database import main as populate_db, clear_database\n",
    "```\n",
    "\n",
    "## Initialize the Database\n",
    "\n",
    "If you haven't already built the database, you can do so from the notebook:\n",
    "\n",
    "\n",
    "```python\n",
    "# Uncomment the next line if you want to reset the database\n",
    "# clear_database()\n",
    "\n",
    "# Build/update the database with documents from the data directory\n",
    "populate_db()\n",
    "```\n",
    "\n",
    "## Query Your Documents\n",
    "\n",
    "Now let's ask some questions about the documents in our knowledge base:\n",
    "\n",
    "\n",
    "```python\n",
    "# Question about Monopoly rules\n",
    "response = query_rag(\"How much money does each player start with in Monopoly?\")\n",
    "print(\"Response:\", response)\n",
    "```\n",
    "\n",
    "Let's try another question:\n",
    "\n",
    "\n",
    "```python\n",
    "# Question about Ticket to Ride\n",
    "response = query_rag(\"How many train cars do players start with in Ticket to Ride?\")\n",
    "print(\"Response:\", response)\n",
    "```\n",
    "\n",
    "## Understanding the Results\n",
    "\n",
    "Each response includes:\n",
    "1. The answer generated by the LLM\n",
    "2. Source references showing which document chunks were used\n",
    "\n",
    "Let's visualize what happens when we ask a question:\n",
    "\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "\n",
    "def visualize_rag_process():\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Define the components\n",
    "    components = ['Documents', 'Chunking', 'Embeddings', 'Vector DB', 'Query', 'Context', 'LLM', 'Response']\n",
    "    y_positions = np.arange(len(components))\n",
    "    \n",
    "    # Plot the components\n",
    "    ax.barh(y_positions, [0.8] * len(components), height=0.6, left=0.1, color='lightgrey', alpha=0.3)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, comp in enumerate(components):\n",
    "        ax.text(0.5, i, comp, va='center', ha='center', fontsize=12)\n",
    "    \n",
    "    # Add flow arrows\n",
    "    for i in range(len(components)-1):\n",
    "        if i < 3:  # Document processing flow\n",
    "            ax.annotate('', xy=(0.9, i), xytext=(0.1, i+1),\n",
    "                        arrowprops=dict(facecolor='blue', shrink=0.05, width=1.5, headwidth=8))\n",
    "        elif i == 3:  # Vector DB to Query\n",
    "            ax.annotate('', xy=(0.5, i), xytext=(0.1, i+1),\n",
    "                        arrowprops=dict(facecolor='green', shrink=0.05, width=1.5, headwidth=8))\n",
    "        else:  # Query to Response flow\n",
    "            ax.annotate('', xy=(0.9, i), xytext=(0.1, i+1),\n",
    "                        arrowprops=dict(facecolor='red', shrink=0.05, width=1.5, headwidth=8))\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.title('RAG System Process Flow', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rag_process()\n",
    "```\n",
    "\n",
    "## Customizing the System\n",
    "\n",
    "You can customize the RAG system by modifying parameters:\n",
    "\n",
    "\n",
    "```python\n",
    "# Example: Custom RAG query with more context chunks\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from get_embedding_function import get_embedding_function\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "def custom_query_rag(query_text, num_chunks=8, temperature=0.1):\n",
    "    # Prepare the DB\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    \n",
    "    # Search the DB with custom parameters\n",
    "    results = db.similarity_search_with_score(query_text, k=num_chunks)\n",
    "    \n",
    "    # Create context from results\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    \n",
    "    # Custom prompt\n",
    "    custom_prompt = \"\"\"\n",
    "    You are an expert research assistant. Answer the question accurately based on the provided context.\n",
    "    If the answer is not in the context, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_template(custom_prompt)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    \n",
    "    # Get response with custom temperature\n",
    "    model = Ollama(model=\"mistral\", temperature=temperature)\n",
    "    response_text = model.invoke(prompt)\n",
    "    \n",
    "    # Get sources\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    \n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"sources\": sources,\n",
    "        \"num_chunks_used\": len(results)\n",
    "    }\n",
    "\n",
    "# Try the custom function\n",
    "result = custom_query_rag(\"What happens when you land on Free Parking?\", num_chunks=10)\n",
    "print(f\"Response: {result['response']}\\n\")\n",
    "print(f\"Number of chunks used: {result['num_chunks_used']}\")\n",
    "print(f\"Sources: {result['sources']}\")\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook showed how to:\n",
    "1. Initialize the RAG database\n",
    "2. Query documents with the standard function\n",
    "3. Visualize the RAG process\n",
    "4. Create a custom query function with different parameters\n",
    "\n",
    "You can extend this system by adding more documents to the `data` directory and rebuilding the database."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
